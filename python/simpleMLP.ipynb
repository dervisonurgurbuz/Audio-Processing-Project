{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract speech.waw and emo tags from datasets\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract multiple acoustic features from audio\n",
    "def extract_features(audio_path):\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "    # Extract different features (MFCCs, chroma, spectral contrast)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "\n",
    "    # Compute statistical features for each feature\n",
    "    mfccs_stats = np.hstack((np.mean(mfccs, axis=1), np.std(mfccs, axis=1)))\n",
    "    chroma_stats = np.hstack((np.mean(chroma, axis=1), np.std(chroma, axis=1)))\n",
    "    spectral_contrast_stats = np.hstack((np.mean(spectral_contrast, axis=1), np.std(spectral_contrast, axis=1)))\n",
    "\n",
    "    # Concatenate multiple features\n",
    "    combined_features = np.hstack((mfccs_stats, chroma_stats, spectral_contrast_stats))\n",
    "\n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data)  # Assuming 'data' is a NumPy array or a list\n",
    "        self.labels = torch.tensor(labels)  # Assuming 'labels' is a NumPy array or a list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate datasets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata    ToyData        angry            xxxx.waw\\n            xxxx.waw\\n            xxxx.waw\\n        happy            xxxx.waw\\n            xxxx.waw\\n            xxxx.waw\\n        neutral            xxxx.waw\\n            xxxx.waw\\n            xxxx.waw\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data\\\n",
    "    ToyData\\\n",
    "        angry\\\n",
    "            xxxx.waw\n",
    "            xxxx.waw\n",
    "            xxxx.waw\n",
    "        happy\\\n",
    "            xxxx.waw\n",
    "            xxxx.waw\n",
    "            xxxx.waw\n",
    "        neutral\\\n",
    "            xxxx.waw\n",
    "            xxxx.waw\n",
    "            xxxx.waw\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing emotion: angry\n",
      "Processing emotion: happy\n",
      "Processing emotion: neutral\n"
     ]
    }
   ],
   "source": [
    "Audio_folder = './data/ToyData/'\n",
    "feature_tensors = []\n",
    "label_tensors = []\n",
    "\n",
    "emo_dict = {'happy': 0, 'angry': 1, 'neutral': 2}\n",
    "\n",
    "for emotion_folder in os.listdir(Audio_folder):\n",
    "    subfolder = os.path.join(Audio_folder, emotion_folder)\n",
    "    if os.path.isdir(subfolder):  # Check if it's a directory\n",
    "        print(f\"Processing emotion: {emotion_folder}\")\n",
    "        # Loop through audio files within each emotion folder\n",
    "        for audio_file in os.listdir(subfolder):\n",
    "            if audio_file.endswith(\".wav\"):  # Ensure the file is a .wav file\n",
    "                file_path = os.path.join(subfolder, audio_file)\n",
    "                features = extract_features(file_path)\n",
    "                feature_tensors.append(features)\n",
    "                label_tensors.append(emo_dict[emotion_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(feature_tensors, label_tensors)\n",
    "train_size = int(0.8 * len(dataset))  \n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# create DataLoader\n",
    "# data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# simple MLP\n",
    "class SimpleMLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.9098253846168518\n",
      "Epoch [2/100], Loss: 0.7968207001686096\n",
      "Epoch [3/100], Loss: 0.8064934611320496\n",
      "Epoch [4/100], Loss: 1.1449300050735474\n",
      "Epoch [5/100], Loss: 0.7718663811683655\n",
      "Epoch [6/100], Loss: 0.670131504535675\n",
      "Epoch [7/100], Loss: 0.7896829843521118\n",
      "Epoch [8/100], Loss: 0.6588603854179382\n",
      "Epoch [9/100], Loss: 0.6428120732307434\n",
      "Epoch [10/100], Loss: 0.7966267466545105\n",
      "Epoch [11/100], Loss: 0.5648611783981323\n",
      "Epoch [12/100], Loss: 0.5816508531570435\n",
      "Epoch [13/100], Loss: 0.5406836867332458\n",
      "Epoch [14/100], Loss: 1.626785159111023\n",
      "Epoch [15/100], Loss: 0.7451679706573486\n",
      "Epoch [16/100], Loss: 0.532037079334259\n",
      "Epoch [17/100], Loss: 0.5537184476852417\n",
      "Epoch [18/100], Loss: 0.43712693452835083\n",
      "Epoch [19/100], Loss: 1.2303595542907715\n",
      "Epoch [20/100], Loss: 0.6777822375297546\n",
      "Epoch [21/100], Loss: 1.0936566591262817\n",
      "Epoch [22/100], Loss: 0.4806949496269226\n",
      "Epoch [23/100], Loss: 0.4495686888694763\n",
      "Epoch [24/100], Loss: 0.4269716739654541\n",
      "Epoch [25/100], Loss: 0.8746917247772217\n",
      "Epoch [26/100], Loss: 0.3380216956138611\n",
      "Epoch [27/100], Loss: 0.4472230076789856\n",
      "Epoch [28/100], Loss: 0.3488122522830963\n",
      "Epoch [29/100], Loss: 0.7040121555328369\n",
      "Epoch [30/100], Loss: 0.7299425601959229\n",
      "Epoch [31/100], Loss: 0.3756869435310364\n",
      "Epoch [32/100], Loss: 0.2907816767692566\n",
      "Epoch [33/100], Loss: 0.47164008021354675\n",
      "Epoch [34/100], Loss: 0.5045366883277893\n",
      "Epoch [35/100], Loss: 0.3310641050338745\n",
      "Epoch [36/100], Loss: 0.5051278471946716\n",
      "Epoch [37/100], Loss: 0.30607253313064575\n",
      "Epoch [38/100], Loss: 0.24372278153896332\n",
      "Epoch [39/100], Loss: 0.3445040285587311\n",
      "Epoch [40/100], Loss: 0.24649082124233246\n",
      "Epoch [41/100], Loss: 0.3143373429775238\n",
      "Epoch [42/100], Loss: 0.31135106086730957\n",
      "Epoch [43/100], Loss: 0.23757998645305634\n",
      "Epoch [44/100], Loss: 0.2343362718820572\n",
      "Epoch [45/100], Loss: 0.31792178750038147\n",
      "Epoch [46/100], Loss: 0.22200115025043488\n",
      "Epoch [47/100], Loss: 0.24928703904151917\n",
      "Epoch [48/100], Loss: 2.3157095909118652\n",
      "Epoch [49/100], Loss: 0.2586519122123718\n",
      "Epoch [50/100], Loss: 0.22426338493824005\n",
      "Epoch [51/100], Loss: 0.23229144513607025\n",
      "Epoch [52/100], Loss: 0.2866688072681427\n",
      "Epoch [53/100], Loss: 0.21093949675559998\n",
      "Epoch [54/100], Loss: 0.2516269385814667\n",
      "Epoch [55/100], Loss: 0.289648175239563\n",
      "Epoch [56/100], Loss: 0.25415605306625366\n",
      "Epoch [57/100], Loss: 0.28668665885925293\n",
      "Epoch [58/100], Loss: 0.1666020154953003\n",
      "Epoch [59/100], Loss: 0.26125985383987427\n",
      "Epoch [60/100], Loss: 0.5983239412307739\n",
      "Epoch [61/100], Loss: 0.20900043845176697\n",
      "Epoch [62/100], Loss: 0.1512509435415268\n",
      "Epoch [63/100], Loss: 0.12338142096996307\n",
      "Epoch [64/100], Loss: 0.1434604376554489\n",
      "Epoch [65/100], Loss: 0.14339298009872437\n",
      "Epoch [66/100], Loss: 0.21005935966968536\n",
      "Epoch [67/100], Loss: 0.12478271126747131\n",
      "Epoch [68/100], Loss: 0.1317550092935562\n",
      "Epoch [69/100], Loss: 0.14830711483955383\n",
      "Epoch [70/100], Loss: 0.14985811710357666\n",
      "Epoch [71/100], Loss: 0.1816886067390442\n",
      "Epoch [72/100], Loss: 0.1683623045682907\n",
      "Epoch [73/100], Loss: 0.12985947728157043\n",
      "Epoch [74/100], Loss: 0.15901926159858704\n",
      "Epoch [75/100], Loss: 0.13150043785572052\n",
      "Epoch [76/100], Loss: 0.10210481286048889\n",
      "Epoch [77/100], Loss: 0.13765764236450195\n",
      "Epoch [78/100], Loss: 0.13434971868991852\n",
      "Epoch [79/100], Loss: 0.11858300119638443\n",
      "Epoch [80/100], Loss: 0.1206149011850357\n",
      "Epoch [81/100], Loss: 0.17534349858760834\n",
      "Epoch [82/100], Loss: 0.09694916754961014\n",
      "Epoch [83/100], Loss: 0.1164180189371109\n",
      "Epoch [84/100], Loss: 0.07440655678510666\n",
      "Epoch [85/100], Loss: 0.12733274698257446\n",
      "Epoch [86/100], Loss: 0.0984911397099495\n",
      "Epoch [87/100], Loss: 0.1564071923494339\n",
      "Epoch [88/100], Loss: 0.0745946615934372\n",
      "Epoch [89/100], Loss: 0.0903063639998436\n",
      "Epoch [90/100], Loss: 0.06569267809391022\n",
      "Epoch [91/100], Loss: 0.10323437303304672\n",
      "Epoch [92/100], Loss: 0.17920874059200287\n",
      "Epoch [93/100], Loss: 0.05601651221513748\n",
      "Epoch [94/100], Loss: 0.07615917176008224\n",
      "Epoch [95/100], Loss: 0.09352128207683563\n",
      "Epoch [96/100], Loss: 0.0747431144118309\n",
      "Epoch [97/100], Loss: 0.08575786650180817\n",
      "Epoch [98/100], Loss: 0.07745396345853806\n",
      "Epoch [99/100], Loss: 0.13017688691616058\n",
      "Epoch [100/100], Loss: 0.11921299248933792\n"
     ]
    }
   ],
   "source": [
    "model = SimpleMLP(input_size=64, hidden_size=32, output_size=3)  \n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "\n",
    "        batch_X, batch_y = batch_X.float(), batch_y.long()  \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 99.17%\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.float(), labels.long()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on test set: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of maximum value: 0 expect:  1\n"
     ]
    }
   ],
   "source": [
    "TestWaw = './data/archive/Crema/1009_IWW_ANG_XX.wav'\n",
    "testFeature = torch.tensor(extract_features(TestWaw))\n",
    "out = model(testFeature.float())\n",
    "max_value = torch.argmax(out)\n",
    "print(\"Index of maximum value:\", max_value.item(), 'expect: ', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of maximum value: 0 expect:  0\n"
     ]
    }
   ],
   "source": [
    "TestWaw = './data/archive/Crema/1001_DFA_HAP_XX.wav'\n",
    "testFeature = torch.tensor(extract_features(TestWaw))\n",
    "out = model(testFeature.float())\n",
    "max_value = torch.argmax(out)\n",
    "print(\"Index of maximum value:\", max_value.item(), 'expect: ', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of maximum value: 2 expect:  2\n"
     ]
    }
   ],
   "source": [
    "TestWaw = './data/archive/Crema/1001_IEO_NEU_XX.wav'\n",
    "testFeature = torch.tensor(extract_features(TestWaw))\n",
    "out = model(testFeature.float())\n",
    "max_value = torch.argmax(out)\n",
    "print(\"Index of maximum value:\", max_value.item(), 'expect: ', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of maximum value: 0 expect:  1\n"
     ]
    }
   ],
   "source": [
    "TestWaw = './data/archive/Crema/1001_ITH_ANG_XX.wav'\n",
    "testFeature = torch.tensor(extract_features(TestWaw))\n",
    "out = model(testFeature.float())\n",
    "max_value = torch.argmax(out)\n",
    "print(\"Index of maximum value:\", max_value.item(), 'expect: ', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of maximum value: 0 expect:  1\n"
     ]
    }
   ],
   "source": [
    "TestWaw = './data/archive/Crema/1003_IEO_ANG_MD.wav'\n",
    "testFeature = torch.tensor(extract_features(TestWaw))\n",
    "out = model(testFeature.float())\n",
    "max_value = torch.argmax(out)\n",
    "print(\"Index of maximum value:\", max_value.item(), 'expect: ', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of maximum value: 2 expect:  2\n"
     ]
    }
   ],
   "source": [
    "TestWaw = './data/archive/Crema/1004_IWW_NEU_XX.wav'\n",
    "testFeature = torch.tensor(extract_features(TestWaw))\n",
    "out = model(testFeature.float())\n",
    "max_value = torch.argmax(out)\n",
    "print(\"Index of maximum value:\", max_value.item(), 'expect: ', 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
